{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8f6926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura de dataset localizada en: /home/adrian/Desktop/Aprendizaje/practica_final/training/dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la ruta del dataset (dentro de la carpeta training)\n",
    "base_dir = '/home/adrian/Desktop/Aprendizaje/practica_final/training'\n",
    "dataset_path = os.path.join(base_dir, 'dataset')\n",
    "\n",
    "# Aseguramos que la estructura sea la correcta (train/images, valid/images)\n",
    "# Esta estructura ya existe en tu carpeta training/dataset\n",
    "print(f\"Estructura de dataset localizada en: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595866fd",
   "metadata": {},
   "source": [
    "# Aplicaci√≥n de Detecci√≥n y Segmentaci√≥n de Productos con YOLO (Transfer Learning)\n",
    "\n",
    "Este notebook contiene el flujo de trabajo para entrenar un modelo YOLO para la detecci√≥n y segmentaci√≥n de productos basados en SKU espec√≠ficos.\n",
    "\n",
    "## 1. Configuraci√≥n del Entorno y Datos\n",
    "\n",
    "Para YOLO, necesitamos una estructura de carpetas espec√≠fica:\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "‚îî‚îÄ‚îÄ labels/\n",
    "    ‚îú‚îÄ‚îÄ train/\n",
    "    ‚îî‚îÄ‚îÄ val/\n",
    "```\n",
    "Cada producto (SKU) se mapear√° a un ID de clase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e8f4fd",
   "metadata": {},
   "source": [
    "## 2. Creaci√≥n del archivo de configuraci√≥n `data.yaml`\n",
    "\n",
    "Este archivo le indica a YOLO d√≥nde encontrar las im√°genes y cu√°les son las etiquetas (SKUs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d0818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo /home/adrian/Desktop/Aprendizaje/practica_final/training/dataset/data.yaml generado corectamente.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# SKUs encontrados en el dataset\n",
    "skus = {\n",
    "    0: 'SKU-LAPTOP-let01',\n",
    "    1: 'SKU-LAPTOP-al01',\n",
    "    2: 'SKU-LAPTOP-asu01',    \n",
    "    3: 'SKU-LAPTOP-mbk01',\n",
    "    \n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    'path': dataset_path,\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'names': skus\n",
    "}\n",
    "\n",
    "# Guardamos el archivo data.yaml dentro de la carpeta del dataset\n",
    "yaml_file_path = os.path.join(dataset_path, 'data.yaml')\n",
    "with open(yaml_file_path, 'w') as f:\n",
    "    yaml.dump(data_config, f)\n",
    "\n",
    "print(f\"Archivo {yaml_file_path} generado corectamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5ac47",
   "metadata": {},
   "source": [
    "## 3. Carga del Modelo y Transfer Learning\n",
    "\n",
    "Cargamos un modelo YOLO pre-entrenado para segmentaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562551de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.8 üöÄ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 7850MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.3, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/adrian/Desktop/Aprendizaje/practica_final/training/dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=15, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.5, mode=train, model=yolo11s-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train16, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/home/adrian/Desktop/Aprendizaje/practica_final/training/runs/segment/train16, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1   1475452  ultralytics.nn.modules.head.Segment          [4, 32, 128, 16, None, [128, 256, 512]]\n",
      "YOLO11s-seg summary: 204 layers, 10,083,836 parameters, 10,083,820 gradients, 33.1 GFLOPs\n",
      "\n",
      "Transferred 555/561 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.10.cv1.conv.weight'\n",
      "Freezing layer 'model.10.cv1.bn.weight'\n",
      "Freezing layer 'model.10.cv1.bn.bias'\n",
      "Freezing layer 'model.10.cv2.conv.weight'\n",
      "Freezing layer 'model.10.cv2.bn.weight'\n",
      "Freezing layer 'model.10.cv2.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.proj.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.pe.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.0.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.1.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.bias'\n",
      "Freezing layer 'model.13.cv1.conv.weight'\n",
      "Freezing layer 'model.13.cv1.bn.weight'\n",
      "Freezing layer 'model.13.cv1.bn.bias'\n",
      "Freezing layer 'model.13.cv2.conv.weight'\n",
      "Freezing layer 'model.13.cv2.bn.weight'\n",
      "Freezing layer 'model.13.cv2.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 639.5¬±102.1 MB/s, size: 118.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/adrian/Desktop/Aprendizaje/practica_final/training/dataset/train/labels.cache... 9910 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9910/9910 2.2Git/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 401.3¬±111.3 MB/s, size: 108.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/adrian/Desktop/Aprendizaje/practica_final/training/dataset/valid/labels.cache... 2816 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2816/2816 227.1Mit/s 0.0s\n",
      "Plotting labels to /home/adrian/Desktop/Aprendizaje/practica_final/training/runs/segment/train16/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.01, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/adrian/Desktop/Aprendizaje/practica_final/training/runs/segment/train16\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss   sem_loss  Instances       Size\n",
      "\u001b[K      1/100       4.8G     0.7372      2.255      2.786      1.204          0        155        640: 11% ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 34/310 1.1it/s 44.0s<4:06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolo11s-seg.pt\u001b[39m\u001b[33m'\u001b[39m) \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Entrenar el modelo (Fine-tuning)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Descomenta la siguiente l√≠nea para iniciar el entrenamiento:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Congela las capas base para Few-Shot Learning\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Aumento de datos agresivo\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmixup\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Mezcla im√°genes para mejorar generalizaci√≥n\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Ideal para segmentaci√≥n de productos\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Detener si deja de mejorar\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModelo cargado y listo para entrenamiento.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:774\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    772\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:244\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:442\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(\u001b[38;5;28mself\u001b[39m.loss).backward()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m     last_opt_step = ni\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:698\u001b[39m, in \u001b[36mBaseTrainer.optimizer_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ema:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/ultralytics/utils/torch_utils.py:651\u001b[39m, in \u001b[36mModelEMA.update\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    648\u001b[39m \u001b[38;5;28mself\u001b[39m.updates += \u001b[32m1\u001b[39m\n\u001b[32m    649\u001b[39m d = \u001b[38;5;28mself\u001b[39m.decay(\u001b[38;5;28mself\u001b[39m.updates)\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m msd = \u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# model state_dict\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ema.state_dict().items():\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v.dtype.is_floating_point:  \u001b[38;5;66;03m# true for FP16 and FP32\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2265\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   2263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   2264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2265\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2269\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_hooks.values():\n\u001b[32m   2271\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2265\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   2263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   2264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2265\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2269\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_hooks.values():\n\u001b[32m   2271\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2265\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   2263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   2264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2265\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2269\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_hooks.values():\n\u001b[32m   2271\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2262\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_pre_hooks.values():\n\u001b[32m   2261\u001b[39m     hook(\u001b[38;5;28mself\u001b[39m, prefix, keep_vars)\n\u001b[32m-> \u001b[39m\u001b[32m2262\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_to_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   2264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Aprendizaje/practica_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2155\u001b[39m, in \u001b[36mModule._save_to_state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars)\u001b[39m\n\u001b[32m   2140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_save_to_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, destination, prefix, keep_vars) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Save module state to the `destination` dictionary.\u001b[39;00m\n\u001b[32m   2142\u001b[39m \n\u001b[32m   2143\u001b[39m \u001b[33;03m    The `destination` dictionary will contain the state\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2153\u001b[39m \u001b[33;03m            module\u001b[39;00m\n\u001b[32m   2154\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2155\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2156\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2157\u001b[39m             destination[prefix + name] = param \u001b[38;5;28;01mif\u001b[39;00m keep_vars \u001b[38;5;28;01melse\u001b[39;00m param.detach()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Al ejecutar esta celda por primera vez, el modelo se descargar√° autom√°ticamente \n",
    "# desde los servidores de Ultralytics si no existe en la carpeta actual.\n",
    "model = YOLO('yolo11s-seg.pt') \n",
    "\n",
    "# Entrenar el modelo (Fine-tuning)\n",
    "# Descomenta la siguiente l√≠nea para iniciar el entrenamiento:\n",
    "model.train(\n",
    "    data=os.path.join(dataset_path, 'data.yaml'), \n",
    "    epochs=100, \n",
    "    imgsz=640, \n",
    "    batch=32,\n",
    "    freeze=15,        # Congela las capas base para Few-Shot Learning\n",
    "    mosaic=1.0,       # Aumento de datos agresivo\n",
    "    mixup=0.5,        # Mezcla im√°genes para mejorar generalizaci√≥n\n",
    "    copy_paste=0.3,   # Ideal para segmentaci√≥n de productos\n",
    "    patience=20       # Detener si deja de mejorar\n",
    ")\n",
    "\n",
    "print(\"Modelo cargado y listo para entrenamiento.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfda043",
   "metadata": {},
   "source": [
    "## 4. Validaci√≥n del Modelo\n",
    "\n",
    "Despu√©s del entrenamiento, es importante evaluar el rendimiento del modelo utilizando el conjunto de datos de validaci√≥n. Esto generar√° m√©tricas como mAP (Precision Promedio Media).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ae1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando modelo entrenado m√°s reciente: /home/adrian/Desktop/Aprendizaje/practica_final/training/runs/segment/train13/weights/best.pt\n",
      "Ultralytics 8.4.8 üöÄ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 7850MiB)\n",
      "YOLO11s-seg summary (fused): 114 layers, 10,068,364 parameters, 0 gradients, 32.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2254.6¬±894.4 MB/s, size: 25.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/adrian/Desktop/Aprendizaje/practica_final/training/dataset/valid/labels... 4 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4/4 1.9Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/adrian/Desktop/Aprendizaje/practica_final/training/dataset/valid/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 7.9it/s 0.1s\n",
      "                   all          4          4      0.665      0.167        0.1     0.0735      0.665      0.167        0.1      0.068\n",
      "      SKU-LAPTOP-let01          1          1          1          0          0          0          1          0          0          0\n",
      "      SKU-LAPTOP-asu01          3          3      0.329      0.333      0.201      0.147      0.329      0.333      0.201      0.136\n",
      "Speed: 0.6ms preprocess, 24.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/adrian/Desktop/Aprendizaje/practica_final/training/runs/segment/val10\u001b[0m\n",
      "\n",
      "M√©tricas de detecci√≥n (Cajas):\n",
      " - mAP@50: 0.100\n",
      " - mAP@50-95: 0.074\n",
      "\n",
      "M√©tricas de segmentaci√≥n (M√°scaras):\n",
      " - mAP@50: 0.100\n",
      " - mAP@50-95: 0.068\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "# Validar el modelo con el set de validaci√≥n definido en data.yaml\n",
    "# Esto usar√° las im√°genes en la carpeta 'valid/images' especificada en el archivo yaml\n",
    "# Encontrar autom√°ticamente el √∫ltimo entrenamiento (train con n√∫mero m√°s alto)\n",
    "train_folders = glob.glob(os.path.join(base_dir, 'runs/segment/train*'))\n",
    "if train_folders:\n",
    "    # Extraer n√∫meros de las carpetas train, train2, train3, etc.\n",
    "    train_numbers = []\n",
    "    for folder in train_folders:\n",
    "        match = re.search(r'train(\\d+)$', folder)\n",
    "        if match:\n",
    "            train_numbers.append((int(match.group(1)), folder))\n",
    "        elif folder.endswith('train'):\n",
    "            train_numbers.append((1, folder))\n",
    "    \n",
    "    # Ordenar y tomar el n√∫mero m√°s alto\n",
    "    if train_numbers:\n",
    "        latest_train = max(train_numbers, key=lambda x: x[0])[1]\n",
    "        model_path = os.path.join(latest_train, 'weights/best.pt')\n",
    "        print(f\"Usando modelo entrenado m√°s reciente: {model_path}\")\n",
    "    else:\n",
    "        model_path = os.path.join(base_dir, 'runs/segment/train/weights/best.pt')\n",
    "else:\n",
    "    model_path = os.path.join(base_dir, 'runs/segment/train/weights/best.pt')\n",
    "\n",
    "model = YOLO(model_path)\n",
    "metrics = model.val()\n",
    "\n",
    "# Mostrar resultados principales\n",
    "print(f\"\\nM√©tricas de detecci√≥n (Cajas):\")\n",
    "print(f\" - mAP@50: {metrics.box.map50:.3f}\")\n",
    "print(f\" - mAP@50-95: {metrics.box.map:.3f}\")\n",
    "\n",
    "if hasattr(metrics, 'seg') and metrics.seg is not None:\n",
    "    print(f\"\\nM√©tricas de segmentaci√≥n (M√°scaras):\")\n",
    "    print(f\" - mAP@50: {metrics.seg.map50:.3f}\")\n",
    "    print(f\" - mAP@50-95: {metrics.seg.map:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a89b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca5a683",
   "metadata": {},
   "source": [
    "## 5. Soluci√≥n Avanzada: Generaci√≥n de Datos Sint√©ticos\n",
    "\n",
    "Si tienes pocas im√°genes (menos de 50 por producto), la mejor soluci√≥n profesional es crear un dataset sint√©tico. Esta celda de c√≥digo te muestra c√≥mo tomar UNA foto de tu producto (con fondo transparente o recortada) y pegarla sobre miles de fondos aleatorios. \n",
    "\n",
    "**Requisitos:**\n",
    "1. Carpeta `backgrounds/` con fotos aleatorias (descarga un pack de internet).\n",
    "2. Carpeta `objects/` con tus productos recortados (formato PNG transparente).\n",
    "\n",
    "Esta t√©cnica es la que usan las grandes empresas de Retail para entrenar sus sistemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_synthetic_image(background_path, object_path, output_path):\n",
    "    \"\"\"\n",
    "    Funci√≥n conceptual para generaci√≥n de datos sint√©ticos.\n",
    "    Toma un fondo y un objeto transparente, y los fusiona.\n",
    "    \"\"\"\n",
    "    bg = cv2.imread(background_path)\n",
    "    obj = cv2.imread(object_path, cv2.IMREAD_UNCHANGED) # Cargar con canal Alpha\n",
    "    \n",
    "    if obj.shape[2] != 4:\n",
    "        print(\"El objeto debe ser PNG con transparencia (4 canales)\")\n",
    "        return\n",
    "\n",
    "    # Redimensionar objeto aleatoriamente\n",
    "    scale = random.uniform(0.3, 0.8)\n",
    "    h, w = obj.shape[:2]\n",
    "    new_size = (int(w*scale), int(h*scale))\n",
    "    obj = cv2.resize(obj, new_size)\n",
    "    \n",
    "    # Calcular posici√≥n aleatoria\n",
    "    # ... (l√≥gica de mezcla alpha blending) ...\n",
    "    # cv2.imwrite(output_path, list_img)\n",
    "    print(\"Imagen sint√©tica generada :)\")\n",
    "\n",
    "# Descomenta para usar cuando tengas tus recortes listos\n",
    "create_synthetic_image('bgs/office.jpg', 'objects/laptop_sku1.png', 'train/images/syn_001.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02f04c",
   "metadata": {},
   "source": [
    "## 5. Predicci√≥n\n",
    "\n",
    "Una vez entrenado, podemos usar el modelo para predecir sobre nuevas im√°genes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5bd1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_product(image_path):\n",
    "    results = model.predict(source=image_path, save=True, show=False)\n",
    "    \n",
    "    for r in results:\n",
    "        # Mostrar imagen con detecciones\n",
    "        im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "        plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# predict_product('ruta/a/tu/imagen.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1fe74",
   "metadata": {},
   "source": [
    "## 6. Prueba en Tiempo Real (C√°mara)\n",
    "\n",
    "Esta funci√≥n te permite abrir la c√°mara de tu laptop para probar el modelo en tiempo real. Se abrir√° una ventana emergente con el video procesado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b092887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√°mara iniciada. Mostrando solo el objeto m√°s probable.\n",
      "Presiona 'q' para cerrar la ventana.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m             cv2.waitKey(\u001b[32m1\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Iniciar c√°mara\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mrun_robust_camera\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_robust_camera\u001b[39m\u001b[34m(confidence_threshold)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     22\u001b[39m         \u001b[38;5;66;03m# Copia para dibujar\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def run_robust_camera(confidence_threshold=0.6):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error al abrir c√°mara\")\n",
    "        return\n",
    "\n",
    "    # Definir un nombre de ventana √∫nico y fijo\n",
    "    WINDOW_NAME = \"Deteccion_YOLO_Segmentacion\"\n",
    "    cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    print(f\"C√°mara iniciada. Mostrando solo el objeto m√°s probable.\")\n",
    "    print(\"Presiona 'q' para cerrar la ventana.\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            \n",
    "            # Copia para dibujar\n",
    "            annotated_frame = frame.copy()\n",
    "            \n",
    "            # Inferencia YOLO\n",
    "            # stream=True es m√°s eficiente para video\n",
    "            results = model(frame, conf=confidence_threshold, verbose=False, stream=False)\n",
    "            \n",
    "            # Tomamos el primer resultado (el frame actual)\n",
    "            r = results[0]\n",
    "            \n",
    "            # Si hay detecciones en el frame\n",
    "            if len(r.boxes) > 0:\n",
    "                # YOLO ordena por confianza, el 0 es el mejor\n",
    "                best_idx = 0\n",
    "                box = r.boxes[best_idx]\n",
    "                conf = float(box.conf[0])\n",
    "                \n",
    "                # Validar confianza\n",
    "                if conf >= confidence_threshold:\n",
    "                    # Clase y Coordenadas\n",
    "                    cls_id = int(box.cls[0])\n",
    "                    label = model.names[cls_id]\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                    \n",
    "                    # --- DIBUJAR SEGMENTACI√ìN (La \"selecci√≥n\" del objeto) ---\n",
    "                    if r.masks is not None:\n",
    "                        # Obtener pol√≠gono de la m√°scara\n",
    "                        mask_coords = r.masks.xy[best_idx].astype(np.int32)\n",
    "                        \n",
    "                        # Crear un overlay semitransparente\n",
    "                        overlay = annotated_frame.copy()\n",
    "                        cv2.fillPoly(overlay, [mask_coords], (0, 255, 0)) # Relleno verde\n",
    "                        # Mezclar original con overlay\n",
    "                        cv2.addWeighted(overlay, 0.4, annotated_frame, 0.6, 0, annotated_frame)\n",
    "                        # Dibujar contorno fino\n",
    "                        cv2.polylines(annotated_frame, [mask_coords], True, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # --- DIBUJAR CAJA Y TEXTO ---\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(annotated_frame, f\"{label} {conf:.2f}\", (x1, y1 - 10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "            # --- CORRECCI√ìN DE VENTANA ---\n",
    "            # Mostramos el frame procesado en la misma ventana pre-configurada\n",
    "            cv2.imshow(WINDOW_NAME, annotated_frame)\n",
    "                \n",
    "            # Salida con 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri√≥ un error: {e}\")\n",
    "    finally:\n",
    "        # Limpieza total para evitar ventanas colgadas\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        # En entornos Linux/Jupyter, a veces se requiere waitKey extra para cerrar\n",
    "        for i in range(5):\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "# Iniciar c√°mara\n",
    "run_robust_camera(confidence_threshold=0.50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
